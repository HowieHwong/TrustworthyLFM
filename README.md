# TrustworthyLFM

## Overall

10 Security and Privacy Problems in Large Foundation Models [Safety, Privacy]




## Large Language Model

Cognitive Overload: Jailbreaking Large Language Models with Overloaded Logical Thinking [Safety]

Adversarial Demonstration Attacks on Large Language Models [Safety]

Instructions as Backdoors: Backdoor Vulnerabilities of Instruction Tuning for Large Language Models [Safety]

Chatgpt as an attack tool: Stealthy textual backdoor attack via blackbox generative model trigger [Safety]

Exploring the limits of domain-adaptive training for detoxifying large-scale language models [Safety]

PromptBench: Towards Evaluating the Robustness of Large Language Models on Adversarial Prompts [Robustness]

On the Safety of Open-Sourced Large Language Models: Does Alignment Really Prevent Them From Being Misused? [Safety]

DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models [Truthfulness, Safety, Fairness, Robustness, Privacy, Machine Ethics]

DP-OPT: Make Large Language Model Your Privacy-Preserving Prompt Engineer [Privacy]

BadChain: Backdoor Chain-of-Thought Prompting for Large Language Models [Safety]

Improving the Robustness of Transformer-based Large Language Models with Dynamic Attention [Robustness]

Exploring the Robustness of Decentralized Training for Large Language Models [Robustness]

BadGPT: Exploring Security Vulnerabilities of ChatGPT via Backdoor Attacks to InstructGPT [Safety]










## Large Vision Model

Semantic Adversarial Attacks via Diffusion Models [Safety]

SneakyPrompt: Jailbreaking Text-to-image Generative Models [Safety]

Trojdiff: Trojan attacks on diffusion models with diverse targets [Safety]



## Large Code Model

## Other Large Foundation Model
