# TrustworthyLFM

## Overall

10 Security and Privacy Problems in Large Foundation Models [Safety, Privacy]




## Large Language Model

Cognitive Overload: Jailbreaking Large Language Models with Overloaded Logical Thinking [Safety]

Adversarial Demonstration Attacks on Large Language Models [Safety]

Instructions as Backdoors: Backdoor Vulnerabilities of Instruction Tuning for Large Language Models [Safety]

Chatgpt as an attack tool: Stealthy textual backdoor attack via blackbox generative model trigger [Safety]

Exploring the limits of domain-adaptive training for detoxifying large-scale language models [Safety]

PromptBench: Towards Evaluating the Robustness of Large Language Models on Adversarial Prompts [Robustness]

On the Safety of Open-Sourced Large Language Models: Does Alignment Really Prevent Them From Being Misused? [Safety]

DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models [Truthfulness, Safety, Fairness, Robustness, Privacy, Machine Ethics]

DP-OPT: Make Large Language Model Your Privacy-Preserving Prompt Engineer [Privacy]

BadChain: Backdoor Chain-of-Thought Prompting for Large Language Models [Safety]

Improving the Robustness of Transformer-based Large Language Models with Dynamic Attention [Robustness]

Exploring the Robustness of Decentralized Training for Large Language Models [Robustness]

BadGPT: Exploring Security Vulnerabilities of ChatGPT via Backdoor Attacks to InstructGPT [Safety]










## Large Vision Model

Semantic Adversarial Attacks via Diffusion Models [Safety]

SneakyPrompt: Jailbreaking Text-to-image Generative Models [Safety]

Trojdiff: Trojan attacks on diffusion models with diverse targets [Safety]

Diffusion Models for Imperceptible and Transferable Adversarial Attack [Safety]

Are diffusion models vulnerable to membership inference attacks? [Safety]

Membership inference attacks against diffusion models [Safety]

Extracting training data from diffusion models [Safety]

Zero-Day Backdoor Attack against Text-to-Image Diffusion Models via Personalization [Safety]

Data-free Black-box Attack based on Diffusion Model [Safety]

Vlattack: Multimodal adversarial attacks on vision-language tasks via pre-trained models [Safety]

Text-to-Image Diffusion Models can be Easily Backdoored through Multimodal Data Poisoning [Safety]

Attack-SAM: Towards Attacking Segment Anything Model With Adversarial Examples [Safety]




## Large Code Model

## Other Large Foundation Model
